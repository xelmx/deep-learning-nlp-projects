{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This initial cell prepares the Python environment for the experiment. First, it installs two critical libraries using `pip`: `transformers` for accessing the pre-trained models from Hugging Face, and `torch`, the underlying deep learning framework. The output of the installation is suppressed with `>> /dev/null` to keep the notebook clean. After the installation, the `time` module is imported to measure model inference speed, and the `collections` module is imported for use in calculating the F1-score metric."
      ],
      "metadata": {
        "id": "cpzq5cjACvm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch >> /dev/null\n",
        "import time\n",
        "import collections"
      ],
      "metadata": {
        "id": "cktuJGBqCtZa"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the specific pre-trained model for the experiment is selected. Three different question-answering models from the Hugging Face Hub are provided as options. For each experimental run, one of the model names is uncommented to set it as the active `model_name`. The selected models represent a range of architectures and sizes: a standard BERT-base model, a smaller DistilBERT model optimized for speed, and a larger BERT-large model known for higher accuracy. This setup allows for a controlled comparison of their performance on the same task."
      ],
      "metadata": {
        "id": "m5n9v0rOpvL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. MODEL SELECTION\n",
        "# Model 1:\n",
        "#model_name = 'deepset/bert-base-cased-squad2'\n",
        "\n",
        "# Model 2:\n",
        "#model_name = 'distilbert-base-cased-distilled-squad'\n",
        "\n",
        "# Model 3:\n",
        "model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'"
      ],
      "metadata": {
        "id": "TdY3gnG8t6DR"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the context paragraph that the question-answering model will use as its source of information. The context is a news article concerning the repatriation of Filipino workers from Lebanon, stored as a single, multi-line string in the `context` variable. The model's task is to extract answers exclusively from this provided text."
      ],
      "metadata": {
        "id": "qEkfjIxHpysz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. DATASET PREPARATION (Contex)\n",
        "context = \"\"\"\n",
        "MANILA – The government is arranging chartered flights for the repatriation of more than 200 overseas Filipino workers in Beirut, Lebanon, the Department of Migrant Workers (DMW) said Wednesday. “We are trying to provide for chartered flights. We’re talking to airline companies so that the chartered flights would be able to accommodate for example, no less than 300 overseas Filipino workers from Beirut,” DMW Undersecretary Bernard Olalia said in a Palace press briefing. This was after the scheduled flights of around 15 OFWs on Sept. 25 were cancelled because of the recent bombings in Beirut. Olalia said around 111 OFWs are staying in four temporary shelters in Beirut and waiting for their repatriation. An additional 110 OFWs are applying for exit permits from the Lebanese government, Olalia said. “Apart from the documented OFWs, we have undocumented OFWs who need to secure travel documents and once they’re given travel documents, we will help them in securing also exit visas or exit permits from the Immigration of the Lebanese government,” he said. Olalia, however, said the Philippine government is facing several challenges, including securing landing rights for chartered flights. He said land and sea routes are being considered, in case the situation escalates and makes it “impossible” to take the air route. “The DMW is also studying the possibility of other routes. Apart from air route, we will be assessing the sea and the land route, should the case or the situation there worsen,” Olalia said. He said the DMW, the Overseas Workers Welfare Administration (OWWA), and other concerned agencies will adopt a “whole-of-government assistance\" upon the directive of President Ferdinand R. Marcos Jr. He said each repatriated OFW will get PHP150,000 in financial assistance from the DMW and OWWA, as well as psychosocial services. Israel has intensified its airstrikes across the northern border into Lebanon, targeting the Iran-backed militant group Hezbollah. Iran fired ballistic missiles in Israel on Tuesday night, following the deadly attacks on Gaza and Lebanon and the recent killings of Hamas, Hezbollah, and Islamic Revolutionary Guard Corps leaders. Olalia said no Filipinos were hurt since the attacks were launched. “We have men on the ground. They work around the clock. At ‘yung mga staff po natin, dinagdagan na po natin (And we augmented our staff) both in Lebanon at (and) nearby posts to be able to provide safest route, to evacuate and ultimately to facilitate the repatriation of our OFWs both either in Lebanon or in Israel,” he said. (PNA)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Bb4kdrChuHPY"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the context defined, the next step is to prepare the questions that the model must answer. A Python list named `questions` is created, containing 10 specific questions. Each question is designed to be directly answerable using only the information available in the `context` article, covering a range of details such as names, numbers, and reasons."
      ],
      "metadata": {
        "id": "C7ftH5qfp0kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (10 Questions) stored in a list.\n",
        "questions = [\n",
        "    \"How many overseas Filipino workers are being repatriated from Beirut, Lebanon?\",\n",
        "    \"Who is the DMW Undersecretary that was quoted in the article?\",\n",
        "    \"Why were the scheduled flights on September 25 cancelled?\",\n",
        "    \"How many OFWs are currently staying in temporary shelters in Beirut?\",\n",
        "    \"What other routes are being considered if air travel becomes impossible?\",\n",
        "    \"Who directed the \\\"whole-of-government assistance\\\" for the OFWs?\",\n",
        "    \"How much financial assistance will each repatriated OFW receive?\",\n",
        "    \"Which militant group is Israel targeting in Lebanon?\",\n",
        "    \"According to the article, were any Filipinos hurt in the attacks?\",\n",
        "    \"What are the undocumented OFWs applying for from the Lebanese government?\"\n",
        "]"
      ],
      "metadata": {
        "id": "soJ3knwqt50Z"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enable a quantitative evaluation of the model, this cell defines the ground-truth answers. A list named `correct_answers` is created, containing the precise, correct answer for each of the 10 questions. This list is parallel to the `questions` list, ensuring that `correct_answers[i]` is the ground truth for `questions[i]`. These answers will be used later to compare against the model's predictions to calculate the Exact Match and F1-Score metrics."
      ],
      "metadata": {
        "id": "PYS9p5Y7p3Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct answers\n",
        "correct_answers = [\n",
        "    \"more than 200\",\n",
        "    \"Bernard Olalia\",\n",
        "    \"the recent bombings in Beirut\",\n",
        "    \"around 111\",\n",
        "    \"land and sea routes\",\n",
        "    \"President Ferdinand R. Marcos Jr.\",\n",
        "    \"PHP150,000\",\n",
        "    \"Hezbollah\",\n",
        "    \"no Filipinos were hurt\",\n",
        "    \"exit permits\"\n",
        "]"
      ],
      "metadata": {
        "id": "kYw_01QfAnCk"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell handles the loading and initialization of the first model, `bert-base-cased-squad2`. It begins by importing the necessary `BertForQuestionAnswering` and `BertTokenizer` classes from the `transformers` library. The `.from_pretrained()` method then downloads and loads the specified model and its tokenizer from the Hugging Face Hub. Finally, these components are assembled into a `question-answering` pipeline. This `qna_pipeline` object simplifies the process of getting predictions from the model."
      ],
      "metadata": {
        "id": "_6-mH8map5QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. MODEL 1 - MODEL AND PIPELINE INITIALIZATION\n",
        "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "print(f\"--- Loading model: {model_name} ---\")\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "qna_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "print(\"--- Model loaded successfully. Starting experiment. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "buoTAiYMut42",
        "outputId": "841d04e1-3e0b-48c6-b2eb-a2b85a98bea5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading model: deepset/bert-base-cased-squad2 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model loaded successfully. Starting experiment. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up the third and final model, `bert-large-uncased-whole-word-masking-finetuned-squad`. Following the established pattern, it imports the `BertForQuestionAnswering` and `BertTokenizer` classes and uses the `.from_pretrained()` method to load this larger, more powerful model and its corresponding tokenizer. The resulting components are then used to create the `qna_pipeline` for this experiment run."
      ],
      "metadata": {
        "id": "EK3k3mG0p9G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. MODEL2 - MODEL AND PIPELINE INITIALIZATION\n",
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "\n",
        "print(f\"--- Loading model: {model_name} ---\")\n",
        "loaded_model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "qna_pipeline = pipeline('question-answering', model=loaded_model, tokenizer=tokenizer)\n",
        "print(\"--- Model loaded successfully. Starting experiment. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdAUrfz4JhsF",
        "outputId": "463961a3-1892-4baa-8a57-a85c1b23b400"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading model: distilbert-base-cased-distilled-squad ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model loaded successfully. Starting experiment. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up the third and final model, `bert-large-uncased-whole-word-masking-finetuned-squad`. Following the established pattern, it imports the `BertForQuestionAnswering` and `BertTokenizer` classes and uses the `.from_pretrained()` method to load this larger, more powerful model and its corresponding tokenizer. The resulting components are then used to create the `qna_pipeline` for this experiment run."
      ],
      "metadata": {
        "id": "vsX_F6dVqGXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. MODEL3 - MODEL AND PIPELINE INITIALIZATION\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
        "print(f\"--- Loading model: {model_name} ---\")\n",
        "loaded_model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "qna_pipeline = pipeline('question-answering', model=loaded_model, tokenizer=tokenizer)\n",
        "print(\"--- Model loaded successfully. Starting experiment. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwHg5iR8LitR",
        "outputId": "830b3360-5369-4f87-9d66-262b5a4a28af"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading model: bert-large-uncased-whole-word-masking-finetuned-squad ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model loaded successfully. Starting experiment. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a crucial helper function, `calculate_f1_score`, for evaluating the model's predictions. The F1-score provides a more nuanced measure of answer quality than a simple exact match. The function works by tokenizing both the predicted and ground-truth answers into words. It then calculates the number of common tokens between them to compute Precision and Recall. Finally, it returns the harmonic mean of these two values, providing a score that reflects the degree of overlap between the predicted and correct answers."
      ],
      "metadata": {
        "id": "IhdTitsGp-lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HELPER FUNCTION TO CALCULATE F1-SCORE\n",
        "def calculate_f1_score(prediction, ground_truth):\n",
        "    \"\"\"Calculates the F1-score between a predicted answer and a true answer.\"\"\"\n",
        "    pred_tokens = prediction.lower().split()\n",
        "    truth_tokens = ground_truth.lower().split()\n",
        "\n",
        "    # if either is empty, F1 is 0\n",
        "    if not pred_tokens or not truth_tokens:\n",
        "        return 0\n",
        "\n",
        "    common_tokens = collections.Counter(pred_tokens) & collections.Counter(truth_tokens)\n",
        "    num_common = sum(common_tokens.values())\n",
        "\n",
        "    precision = num_common / len(pred_tokens)\n",
        "    recall = num_common / len(truth_tokens)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "metadata": {
        "id": "MYoqUqSqBT5K"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main execution block of the experiment. It iterates through each of the 10 questions using a `for` loop. Inside the loop, for each question, the following steps are performed: the start time is recorded; the `qna_pipeline` is called with the question and context to get a prediction; and the end time is recorded to calculate the `inference_time`. After extracting the predicted answer text and its confidence score from the result object, two key metrics are calculated: Exact Match (EM), a binary score of 1 or 0, and the F1-score, using the helper function defined previously. All of these results—the predicted answer, inference time, confidence score, EM, and F1-score—are then printed in a structured format for each question."
      ],
      "metadata": {
        "id": "HZN6bbJNqAKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. EXPERIMENT EXECUTION\n",
        "for i, question in enumerate(questions):\n",
        "    print(f\"\\nQ{i+1}: {question}\")\n",
        "\n",
        "    # Get the correct answer for this question\n",
        "    true_answer = correct_answers[i]\n",
        "    print(f\"Correct Answer: {true_answer}\")\n",
        "\n",
        "    # Start timer\n",
        "    start_time = time.time()\n",
        "    # Get the answer from the pipeline\n",
        "    answer_obj = qna_pipeline({'question': question, 'context': context})\n",
        "    # End timer and calculate duration\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Extract predicted answer text\n",
        "    predicted_answer = answer_obj['answer']\n",
        "\n",
        "    # Calculate metrics\n",
        "    confidence_score = answer_obj['score']\n",
        "    exact_match = 1 if predicted_answer.lower() == true_answer.lower() else 0\n",
        "    f1 = calculate_f1_score(predicted_answer, true_answer)\n",
        "\n",
        "    # Print all the results needed for your Excel sheet\n",
        "    print(f\"Model's Answer: {predicted_answer}\")\n",
        "    print(f\"Inference Time (s): {inference_time:.4f}\")\n",
        "    print(f\"Confidence Score: {confidence_score:.4f}\")\n",
        "    print(f\"Exact Match (EM): {exact_match}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aWCQ64NxAY_u",
        "outputId": "fb620902-8756-4f49-84a0-ee59d9d78bde"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q1: How many overseas Filipino workers are being repatriated from Beirut, Lebanon?\n",
            "Correct Answer: more than 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Answer: 300\n",
            "Inference Time (s): 28.0460\n",
            "Confidence Score: 0.1870\n",
            "Exact Match (EM): 0\n",
            "F1-Score: 0.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Q2: Who is the DMW Undersecretary that was quoted in the article?\n",
            "Correct Answer: Bernard Olalia\n",
            "Model's Answer: Bernard Olalia\n",
            "Inference Time (s): 35.2427\n",
            "Confidence Score: 0.9864\n",
            "Exact Match (EM): 1\n",
            "F1-Score: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Q3: Why were the scheduled flights on September 25 cancelled?\n",
            "Correct Answer: the recent bombings in Beirut\n",
            "Model's Answer: recent bombings in Beirut.\n",
            "Inference Time (s): 16.9388\n",
            "Confidence Score: 0.3211\n",
            "Exact Match (EM): 0\n",
            "F1-Score: 0.6667\n",
            "--------------------------------------------------\n",
            "\n",
            "Q4: How many OFWs are currently staying in temporary shelters in Beirut?\n",
            "Correct Answer: around 111\n",
            "Model's Answer: 111\n",
            "Inference Time (s): 16.6202\n",
            "Confidence Score: 0.6898\n",
            "Exact Match (EM): 0\n",
            "F1-Score: 0.6667\n",
            "--------------------------------------------------\n",
            "\n",
            "Q5: What other routes are being considered if air travel becomes impossible?\n",
            "Correct Answer: land and sea routes\n",
            "Model's Answer: land and sea routes\n",
            "Inference Time (s): 16.5125\n",
            "Confidence Score: 0.7023\n",
            "Exact Match (EM): 1\n",
            "F1-Score: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Q6: Who directed the \"whole-of-government assistance\" for the OFWs?\n",
            "Correct Answer: President Ferdinand R. Marcos Jr.\n",
            "Model's Answer: President Ferdinand R. Marcos Jr.\n",
            "Inference Time (s): 16.8384\n",
            "Confidence Score: 0.7365\n",
            "Exact Match (EM): 1\n",
            "F1-Score: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Q7: How much financial assistance will each repatriated OFW receive?\n",
            "Correct Answer: PHP150,000\n",
            "Model's Answer: PHP150,000\n",
            "Inference Time (s): 17.2285\n",
            "Confidence Score: 0.9638\n",
            "Exact Match (EM): 1\n",
            "F1-Score: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Q8: Which militant group is Israel targeting in Lebanon?\n",
            "Correct Answer: Hezbollah\n",
            "Model's Answer: Hezbollah.\n",
            "Inference Time (s): 33.2172\n",
            "Confidence Score: 0.8807\n",
            "Exact Match (EM): 0\n",
            "F1-Score: 0.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Q9: According to the article, were any Filipinos hurt in the attacks?\n",
            "Correct Answer: no Filipinos were hurt\n",
            "Model's Answer: no Filipinos were hurt\n",
            "Inference Time (s): 16.4144\n",
            "Confidence Score: 0.3935\n",
            "Exact Match (EM): 1\n",
            "F1-Score: 1.0000\n",
            "--------------------------------------------------\n",
            "\n",
            "Q10: What are the undocumented OFWs applying for from the Lebanese government?\n",
            "Correct Answer: exit permits\n",
            "Model's Answer: exit permits\n",
            "Inference Time (s): 17.8357\n",
            "Confidence Score: 0.8919\n",
            "Exact Match (EM): 1\n",
            "F1-Score: 1.0000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}